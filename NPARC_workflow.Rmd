---
title: "Non-parametric analysis of thermal proteome profiles"
author: "Dorothee Childs, Nils Kurzawa"
date: "`r format(Sys.time(), '%d %B %Y,   %X')`"
bibliography: bibliography.bib
output: 
  BiocStyle::pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = FALSE)
```

# Introduction
This workflow shows how to reproduce the analysis described by [Childs, Bach, Franken et al. (2018): Non-parametric analysis of thermal proteome profiles reveals novel drug-binding proteins.](https://www.biorxiv.org/content/early/2018/07/22/373845)

# Preparation

Load necessary packages:
```{r dependencies, message=FALSE}
library(tidyverse)
library(broom)
library(knitr)
```

# Data import

First we load the data from the different TPP experiments. All data have been downloaded from the supplements of the respective publications [@Franken2015, @Reinhard2015, @Savitski2014], converted into tidy format, and concatenated into one table. This table will be made available as supplementary material to the paper. Until then, it can be found in the same folder as this vignette.

```{r load_data}
tppData <- readRDS("tppData.Rds")
```

Let's take a look at the first lines of the imported data:
```{r data_head}
tppData %>% head %>% kable()
```

We can see that the data contains the following columns:

- `dataset`: The dataset containing the measurements of several TMT-10 experiments. In each experiment, cells were treated with a vehicle or with the compound in one or two concentrations, and measured at ten different temperatures.
- `uniqueID`: The unique identifier for each protein. Depending on the dataset, it either contains the gene symbol, or the gene symbol concatenated by IPI id.
- `relAbundance`: The relative signal intensity of the protein in each experiment, scaled to the intensity at the lowest temperature.
- `temperature`: The temperatures corresponding to each of the ten measurements in a TMT experiment.
- `compoundConcentration` The concentration of the administered compound in $\mu M$.
- `replicate`: The replicate number in each experimental group. Each pair of vehicle and treatment experiments was conducted in two replicates.
- `uniquePeptideMatches`: The number of unique peptides with which a protein was identified.

And a data summary:
```{r summarize_data}
tppData %>% 
  mutate(compoundConcentration = factor(compoundConcentration), 
         replicate = factor(replicate), 
         dataset = factor(dataset)) %>% 
  summary()
```

# Data preprocessing

Remove all decoy proteins remaining in the panobinostat data. They can be recognized by the prefix `###`, which was assigned by the quantification software `isobarQuant`.

```{r remove_decoys, results='asis'}
tppData <- tppData %>% filter(!grepl("###[[:alnum:]]*###", uniqueID))
```

Remove all proteins that were not found with at least one unique peptide.
```{r qupm_filter, results='asis'}
tppData <- filter(tppData, uniquePeptideMatches >= 1)
```

Re remove all proteins that only contain missing values.
```{r remove_NAs, results='asis'}
tppData <- tppData %>% filter(!is.na(relAbundance))
```

Remove all proteins not reproducibly observed with full melting curves in both replicates and treatment groups per dataset.
A full melting curve is defined by the presence of measurements at all 10 temperatures for the given experimental group.

```{r rm_non_reproducibles, results='asis'}
tppData <- tppData %>%
  group_by(dataset, uniqueID) %>%
  mutate(n = n()) %>%
  group_by(dataset) %>%
  mutate(max_n = max(n)) %>% 
  filter(n == max_n) %>%
  dplyr::select(-n, -max_n) %>%
  ungroup
```

## Reproduce Table 1 of the paper
Count the numbers of proteins remaining in each dataset. They coincide with the values reported in Table 1.

```{r count_all_proteins}
tppData %>% 
  distinct(dataset, uniqueID) %>% 
  distinct %>% 
  group_by(dataset) %>% 
  tally %>%
  kable()
```

# Illustrative example

We first illustrate the principles of nonparametric analysis of response curves (NPARC) on an example protein (STK4) from the staurosporine dataset. The same protein is shown in Figures 1 and 2 of the paper.

## Select data

We first select all data entries belonging to the desired protein and dataset:
```{r select_stk4}
stk4 <- filter(tppData, dataset == "Staurosporine", uniqueID == "STK4_IPI00011488")
```

The table `stk4` has `r nrow(stk4)` rows that contain measurements observed in four experimental groups. They consist of two treatment groups (vehicle: 0 muM staurosporine, treatment: 20 muM staurosporine) with two replicates each. Let us look at the treatment group of replicate 1 for an exaple:
```{r}
stk4 %>% filter(compoundConcentration == 20, replicate == 1) %>% kable()
```


To obtain a first impression of the measurements in each experimental group, we generate a plot of the measurements:
```{r plot_stk4}
stk4_plot <- ggplot(stk4, aes(x = temperature, y = relAbundance)) +
  geom_point(aes(shape = factor(replicate), color = factor(compoundConcentration))) +
  theme_bw() +
  ggtitle("STK4") +
  scale_color_manual("molar staurosporine concentration", 
                     values = c("#808080", "#da7f2d"))

print(stk4_plot)
```
We will show how to add the fitted curves to this plot in the following steps.

## Define function for model fitting
To assess whether there is a significant difference between both treatment groups, we will fit a null model and an alternative models to the data. The null model fits a sigmoid melting curve through all data points irrespective of experimental condition. The alternative model fits separate melting curves per experimental group .

Because we have to repeat the fitting several times in this workflow, we define a function that we can call repeatedly:
```{r define_fit_fct}
fitSingleSigmoid <- function(x, y, start=c(Pl = 0, a = 550, b = 10)){
  try(nls(formula= y ~ (1 - Pl)  / (1+exp((b - a/x))) + Pl, 
          start=start, 
          data=list(x=x, y=y),
          na.action = na.exclude, 
          algorithm = "port",
          lower = c(0.0, 1e-5, 1e-5), 
          upper = c(1.5, 15000, 250),
          control = nls.control(maxiter=50)), 
      silent = TRUE)
}
```

## Fit null models

Now, we can use the function defined in the previous Section to fit the null model:

```{r fit_null_stk4}
nullFit <- fitSingleSigmoid(x = stk4$temperature, y = stk4$relAbundance)
```

The function returns an object of class `nls` and we can display the results by the `summary()` function:
```{r summarize_null_stk4}
summary(nullFit)
```


The underlying data is contained in a nested list. The function `augment` from the `broom` package provides a convenient way to obtain the predictions and residuals at each temperature in tabular format. By appending the returned predictions and residuals to our measurements, we ensure that relevant data is collected in the same table and can be added to the plot for visualization. The residuals will be needed later for construction of the test statistic:

```{r augment_null_stk4}
nullPredictions <- broom::augment(nullFit)
```

Let us look at the values returned by `augment` at two consecutive temperatures. Note that, while the predictions will be the same for each experiment at a given temperature, the residuals will differ because they were computed by comparing the predictions to the actual measurements:
```{r head_augment_result}
nullPredictions %>% filter(x %in% c(46, 49)) %>% kable()
```


Now we can append these values to our data frame and show the predicted curve in the plot:
```{r add_null_resids_stk4}
stk4$nullPrediction <- nullPredictions$.fitted
stk4$nullResiduals <- nullPredictions$.resid

stk4_plot <- stk4_plot + geom_line(data = stk4, aes(y = nullPrediction))

print(stk4_plot)
```

## Fit alternative models

Next we fit the alternative model. Again, we compute the predicted values and the corresponding residuals by the `broom::augment()` function. To take the compound concentration as a factor into account, we iterate over both concentrations and fit separate models to each subset. We implement this by first grouping the data using the function `dplyr::group_by()`, and starting the model fitting by `dplyr::do()`.

```{r fit_alternative_stk4}
alternativePredictions <- stk4 %>%
# Fit separate curves per treatment group:
  group_by(compoundConcentration) %>%
  do({
    fit = fitSingleSigmoid(x = .$temperature, 
                           y = .$relAbundance)
    broom::augment(fit)
  }) %>%
  ungroup %>%
  # Rename columns for merge to data frame:
  dplyr::rename(alternativePrediction = .fitted,
                alternativeResiduals = .resid,
                temperature = x,
                relAbundance = y)
```

Add the predicted values and corresponding residuals to our data frame:
```{r add_alternative_resids_stk4}
stk4 <- stk4 %>%
  left_join(alternativePredictions, 
            by = c("relAbundance", "temperature", 
                   "compoundConcentration")) %>%
  distinct()
```


## Reproduce Figure 2 (A)/(B) of the paper
Add the curves predicted by the alternative model to the plot:
```{r plot_null_alternative_stk4}
stk4_plot <- stk4_plot +
  geom_line(data = distinct(stk4, temperature, compoundConcentration, alternativePrediction), 
            aes(y = alternativePrediction, color = factor(compoundConcentration)))

print(stk4_plot)
```

This plot corresponds to Figures 2(A) and 2(B) in the paper.

## Compute RSS values

In order to quantify the improvement in goodness-of-fit of the alternative model relative to the null model, we compute the sum of squared residuals (RSS):

```{r compute_rss_stk4}
rssPerModel <- stk4 %>%
  summarise(rssNull = sum(nullResiduals^2),
            rssAlternative = sum(alternativeResiduals^2))

kable(rssPerModel, digits = 4)
```

These values will be used to construct the F-statistic according to

\begin{equation}
\label{eq:f_stat}
    \operatorname{F} = \frac{\operatorname{DOF}_{2}}{\operatorname{DOF}_{1}} \cdot \frac{\operatorname{RSS}^{0} - \operatorname{RSS}^{1}}{\operatorname{RSS}^{1}}.
\end{equation}

To compute this statistic and to derive a p-value, we need the degrees of freedom $\operatorname{DOF}_{1}$ and $\operatorname{DOF}_{2}$. As described in the paper, they cannot be analytically derived due to the correlated nature of the measurements. The paper describes how to estimate these values from the RSS-values of all proteins in the dataset. In the following Section, we illustrate how to repeat the model fitting for all proteins of a dataset and how to perform hypothesis testing on these models.

# Extending the analysis to all proteins 

In order to analyze all datasets as described in the paper, we fit null and alternative models to all proteins in each dataset, as shown in the following.

Before starting the model fitting, we combine both dasatinib datasets into one dataset with four replicates of the vehicle experiments, and two replicates in each of two treatment groups. In one treatment group, dasatinib was administered with 0.5 $\mu M$ concentration, and in the other group with $5 \mu M$. 

```{r collate_dasatinib_datasets}
# Remove suffix from dataset names that distinguishes both dasatinib datasets
tppData <- tppData %>%
  mutate(replicate = ifelse(dataset == "Dasatinib 5", 
                            yes = replicate + 2,
                            no = replicate)) %>%
  mutate(dataset = gsub(" 0.5| 5", "", dataset))

# Check result: List all dataset names and the administered drug concentrations
tppData %>% 
  distinct(dataset, replicate, compoundConcentration) %>% 
  filter(compoundConcentration > 0) %>%
  dplyr::rename(`drug concentration (treatment groups)` = compoundConcentration) %>%
  kable()
```

## Define functions
We fit the models by the same function as illustrated on the STK4 example above. In order to iterate over all proteins and treatment groups, we split the data by the `dplyr::group_by()` function, and loop over all subsets by the `dplyr::do()` function. For each model, we retrieve the residuals by the function `residuals()` and compute the sum of their squared values (RSS). We encapsulate this code into a function that we can re-use for the null and alternative model fits of each protein. It will also make debugging easier if the code lives within a separate function.

For a few proteins, the nonlinear least-squares optimization will not converge with the given start parameters. For some of these proteins, however, convergence can be obtained after adding a small random noise to the start paramters. To this purpose, we write a wrapper around fitSingleSigmoid()` that starts the optimization repeatedly with randomly perturbed start parameters for such proteins:

```{r define_repeatFits}
repeatFits <- function(x, y, seed = NULL, alwaysPermute = FALSE, maxAttempts = 100){
  
  start <- c(Pl = 0, a = 550, b = 10)
  i <- 0
  doFit <- TRUE
  doVaryPars <- alwaysPermute
  
  if (!is.null(seed)){
    set.seed(seed)
  }
  
  while (doFit){
    startTmp <- start * (1 + doVaryPars*runif(1, -0.5, 0.5))
    m <- fitSingleSigmoid(x = x, y = y, start = startTmp)
    i <- i + 1
    doFit <- inherits(m, "try-error") & i < maxAttempts
    doVaryPars <- TRUE
  }
  
  return(m)
}
```


```{r define_fct_returnPredictions}
computeRSS <- function(x, y, seed = NULL, alwaysPermute = FALSE, maxAttempts = 100){
  
  # Start model fitting
  fit <- repeatFits(x = x, y = y, seed = seed, 
                    alwaysPermute = alwaysPermute, 
                    maxAttempts = maxAttempts)
  
  if (!inherits(fit, "try-error")){
    # If model fit converged, obtain data frame containing predicted values and residuals
    resid <- residuals(fit)
    rss <- sum(resid^2, na.rm = TRUE)
    fittedValues <- sum(!is.na(resid))
  } else {
    # If model fit did not converge, return default values
    rss <- NA
    fittedValues <- 0
  }
  
  return(data.frame(rss = rss, fittedValues = fittedValues))
}
```



## Fit null models

Now we can fit the null models to each protein in each dataset:

```{r, echo = FALSE}
# debug(returnPredictions)
# debug(repeatFits)
```

```{r fit_null_all_datasets, warning=FALSE, cache=TRUE}
nullRSS <- tppData %>%
  group_by(dataset, uniqueID) %>%
  do(
    computeRSS(x = .$temperature, y = .$relAbundance, seed = 123)
    ) %>%
  ungroup
```

Show a data summary:
```{r summarize_null_rss}
nullRSS %>%
  mutate(dataset = factor(dataset), fittedValues = factor(fittedValues)) %>%
  summary()
```

Determine the maximum number of measurements possible per protein in each dataset.
We will need this information to detect those proteins for which the model converged in each experimental group.
```{r}
maxFitted <- nullRSS %>% 
  group_by(dataset) %>%
  summarise(maxDataPoints = max(fittedValues, na.rm = TRUE))
```


<!-- ```{r, eval = FALSE, echo = FALSE} -->
<!-- # List all proteins for which the model fit did not converge: -->
<!-- nullRSS %>% filter(!isConverged) -->
<!-- ``` -->


## Fit alternative models

Next we fit the alternative models:
```{r fit_alternative_all_datasets, warning=FALSE, cache=TRUE}
alternativeRSS <- tppData %>%
  group_by(dataset, uniqueID, compoundConcentration) %>%
  do(
    computeRSS(x = .$temperature, y = .$relAbundance, seed = 123)
    ) %>%
  ungroup
```

Show a data summary:
```{r summarize_alternative_rss}
alternativeRSS %>%
  mutate(dataset = factor(dataset), fittedValues = factor(fittedValues)) %>%
  summary()
```


Count lengths of fitted curves occuring per experiment:
```{r}
alternativeRSS %>%
  group_by(dataset, fittedValues, compoundConcentration) %>%
  tally() %>%
  kable()
```

Compute the final RSS values per protein as the sum of the RSS values in each treatment group:
```{r collate_alternative_rss}
alternativeRSSCollated <- alternativeRSS %>%
  group_by(dataset, uniqueID) %>% 
  summarise(rss = sum(rss, na.rm = TRUE), 
            fittedValues = sum(fittedValues))
```


## Combine results from both model fits
Combine the RSS values of all proteins for which the models converged in all groups:
```{r combineRSS}
# Make unique columns for merge to a common data frame:
dat1 <- nullRSS %>% 
  dplyr::rename(rss0 = rss, n0 = fittedValues)

dat2 <- alternativeRSSCollated %>% 
  dplyr::rename(rss1 = rss, n1 = fittedValues)

# Perform merge of both tables:
allRSS <- full_join(dat1, dat2, 
                    by = c("dataset", "uniqueID")) %>%
  # Select proteins for which models converged in each treatment group:
  left_join(maxFitted, by = "dataset") %>%
  mutate(allConverged = (n0 == maxDataPoints & 
                           n1 == maxDataPoints)) %>%
  dplyr::select(-maxDataPoints)
```

In order to quantify the improvement in goodness-of-fit of the alternative model relative to the null model, we compute the difference in RSS between both models for each protein. Proteins for which $\operatorname{RSS}^1$ is not smaller than $\operatorname{RSS}^0$ are excluded from p-value calculation.
```{r computeRSSdiff}
allRSS <- allRSS %>%
  mutate(rssDiff = rss0 - rss1) %>%
  mutate(rssDiff = ifelse(rssDiff < 0, NA, rssDiff)) %>%
  mutate(applicableForTesting = allConverged & !is.na(rssDiff))
```


Data summary of proteins for which we could compute valid RSS differences per dataset:
```{r summarizePredictions}
allRSS %>%
  filter(applicableForTesting) %>%
  mutate(dataset = factor(dataset), n0 = factor(n0), n1 = factor(n1)) %>%
  summary()
```

<!-- Which proteins are affected? -->
<!-- ```{r showNegativeRSSdiffs} -->
<!-- allRSS %>% -->
<!--   filter(rssDiff < 0) %>% -->
<!--   filter(dataset %in% c("Panobinostat", "Staurosporine")) %>% -->
<!--   kable() -->
<!-- ``` -->

<!-- Remove these proteins: -->
<!-- ```{r filterRSS} -->
<!-- # rssFiltered <- allRSS %>% filter(rssDiff > 0) -->
<!-- ``` -->

<!-- Let us look at the first entries and at the column summaries of the data frame `allRSS`: -->

<!-- ```{r head_rssPerModel} -->
<!-- allRSS %>%  -->
<!--   head %>%  -->
<!--   kable(digits = 4) -->
<!-- ``` -->

<!-- <!-- ## Compare to results published in the manuscript -->
<!-- ```{r load_paper_results, echo = FALSE, eval=FALSE} -->
<!-- #  for comparison: load in-house code results as reported in the paper -->
<!-- f_paper_results <- "/Users/dorotheechilds/Work/Projects/Thermal_proteome_profiling/Gitlab_repositories/Gitlab_paper_nonparametric_TPP/master/04 Data/cache/NPARC_results_min_qupm=1.Rds" -->

<!-- paperResults <- readRDS(f_paper_results) -->

<!-- paperResults <- paperResults %>% -->
<!--   dplyr::select(dataset = datasetID, uniqueID, nPaper = fitted_values, rssNullPaper = rss0, rssAlternativePaper = rss1) %>% -->
<!--   mutate(rssDiffPaper = rssNullPaper - rssAlternativePaper) %>% -->
<!--   mutate(dataset = plyr::mapvalues(dataset, -->
<!--                                    c("ATP_Reinhardetal_2015_PBS", "Dasatinib_Savitskietal_2014", "Panobinostat_Frankenetal_2015", "Staurosporine_Savitskietal_2014"), -->
<!--                                    c("ATP", "Dasatinib", "Panobinostat", "Staurosporine"))) %>% -->
<!--   filter(dataset %in% c("ATP", "Dasatinib", "Panobinostat", "Staurosporine")) -->

<!-- paperResults %>% -->
<!--   mutate(dataset = factor(dataset), -->
<!--          nPaper = factor(nPaper)) %>% -->
<!--   filter(!is.na(nPaper)) %>% -->
<!--   summary(digits = 2) %>%  -->
<!--   kable() -->
<!-- ``` -->

<!-- ```{r add_paper_results, echo = FALSE, eval=FALSE} -->
<!-- comparison <- full_join(rssPerModel, paperResults) %>%   -->
<!--   mutate(diffToPaperNull = rssNullPaper - rssNull) %>% -->
<!--   mutate(diffToPaperAlternative = rssAlternativePaper - rssAlternative)  -->
<!-- ``` -->

<!-- ```{r find_diffs_to_paper_results, echo = FALSE, eval=FALSE} -->
<!-- comparison %>% -->
<!--   arrange(-abs(diffToPaperNull), - abs(diffToPaperAlternative)) %>% -->
<!--   head(10) -->


<!-- comparison %>% -->
<!--   filter(is.na(diffToPaperNull)) %>% -->
<!--   filter(dataset == "Panobinostat") -->
<!-- ``` -->

<!-- ```{r plot_diffs_to_paper_results, echo = FALSE, eval=FALSE} -->
<!-- datTmp <- filter(tppData, uniqueID == "RRP9_NA", dataset == "Panobinostat") -->
<!-- predTmp <- filter(allPredictions, uniqueID == "RRP9_NA", dataset == "Panobinostat") -->

<!-- ggplot(predTmp, aes(x = temperature, y = relAbundance)) + -->
<!--   geom_point(aes(shape = factor(replicate), color = factor(compoundConcentration))) + -->
<!--   geom_line(aes(y = alternativePrediction, color = factor(compoundConcentration))) + -->
<!--   theme_bw() + -->
<!--   ggtitle("RRP9_NA") + -->
<!--   scale_color_manual("molar panobinostat concentration",  -->
<!--                      values = c("1e-06" = "#da7f2d", -->
<!--                                 "0" = "#808080")) -->
<!-- ``` -->


## Compute test statistics

### Why we need to estimate the degrees of freedom

In order to compute F-statistics per protein and dataset according to Equation (\ref{eq:f_stat}), we need to know the degrees of freedom of the corresponding null distribution. If we could assume independent and identically distributed (iid) residuals, we could compute them from the number of fitted values and model parameters. In the following, we will show why this simple equation is not appropriate for the curve data we are working with.

First, we compute the degrees of freedom that we would assume for iid residuals:
```{r computeDOFiid}
DOF <- allRSS %>%
  filter(applicableForTesting) %>%
  mutate(paramsNull = 3,
         paramsAlternative = ifelse(n1 > 40, yes = 9, no = 6)) %>%
  mutate(DOF1 = paramsAlternative - paramsNull,
         DOF2 = n1 - paramsAlternative)
```

Let us take a look at the computed degrees of freedom:
```{r showDOFiid}
DOF %>% 
  filter(allConverged) %>%
  distinct(dataset, n0, n1, paramsNull, paramsAlternative, DOF1, DOF2) %>%
  kable()
```

No we calculate the F-statistics per protein and compare them to the corresponding F-distribution to derive p-values:
```{r computeFiid}
testResults <- DOF %>%
  mutate(fStat = (rssDiff/DOF1) / (rss1/DOF2),
         pVal = 1 - pf(fStat, df1 = DOF1, df2 = DOF2),
         pAdj = p.adjust(pVal, "BH"))
```

We plot the F-statistics against the theoretical F-distribution to check how well the null distribution is approximated now:

```{r plotFiid}
ggplot(testResults) +
  geom_density(aes(x = fStat), fill = "steelblue", alpha = 0.5) +
  geom_line(aes(x = fStat, y = df(fStat, df1 = DOF1, df2 = DOF2)), color = "darkred", size = 1.5) +
  facet_wrap(~ dataset + n0 + n1) +
  theme_bw() +
  # Zoom in to small values to increase resolution for the proteins under H0:
  xlim(c(0, 10))
```

The densities of the theoretical F-distribution (red) do not fit the observed values (blue) very well. While the theoretical distribution tends to overestimate the number of proteins with test statistics smaller than 2.5, it appears to underestimate the amount of proteins with larger values.  This would imply that even for highly specific drugs, we observe many more significant differences than we would expect by chance. This hints at an anti-conservative behaviour of our test with the calculated degree of freedom parameters. This is reflected in the p-value distributions. If the distribution assumptions were met, we would expect the null cases to follow a uniform distribution, with a peak on the left for the non-null cases. Instead, we observe a tendency to obtain fewer values than expected in the middle range (around 0.5), but distinct peaks to the left.

```{r plotPiid}
ggplot(testResults) +
  geom_histogram(aes(x = pVal, y = ..density..), fill = "steelblue", alpha = 0.5, boundary = 0) +
  geom_line(aes(x = pVal, y = dunif(pVal)), color = "darkred", size = 1.5) +
  facet_wrap(~ dataset + n0 + n1) +
  theme_bw()
```


### How to estimate the degrees of freedom

In the paper, we describe an alternative way to estimate the degrees of freedom by fitting $\chi^2$ distributions to the numerator and denominator across all proteins in a dataset. To enable fitting of the distributions, we first need to re-scale the variables by a scaling factor. Because the scaling factors are characteristic for each dataset (it depends on the variances of the residuals in the respective dataset), we estimate them from the data according to:

\begin{align} \label{eq:scale-param}
\sigma_0^2 &= \frac{1}{2} \frac{V}{M},
\end{align}

where $V$ is the variance of the distribution, and $M$ is the mean of the distribution.

We estimate $V$ and $M$ from the empirical distributions of the RSS differences $(\operatorname{RSS}^1 - \operatorname{RSS}^0)$. To increase robustness, we estimate $M$ and $V$ by their D-estimates @Marazzi2002 (median and median absolute deviation). 

```{r estimate_scaling_factors}
scalingFactors <- allRSS %>%
  filter(applicableForTesting) %>%
  group_by(dataset) %>%
  summarise(M = median(rssDiff, na.rm = T), V = mad(rssDiff, na.rm = T)^2) %>%
  ungroup %>%
  mutate(s0_sq = 1/2 * V/M)

scalingFactors %>% kable()
```

We scale the numerator and denominator of the F-statistic by these scaling factors and estimate the degree of freedom parameters by fitting unscaled $\chi^2$ distributions.

First we add the scaling factors to the filtered RSS data as a separate column:
```{r scaled_rss}
rssScaled <- scalingFactors %>%
  dplyr::select(dataset, s0_sq) %>%
  left_join(allRSS, by = "dataset") %>%
  mutate(rssDiff = rssDiff/s0_sq,
         rss1 = rss1/s0_sq)
```

Then we fit the degrees of freedom parameters numerically. This estimation proves to be fairly robust regarding the choice of the initial values, so we choose a small value of 1 for each optimization.

```{r estimate_DOF, warning=FALSE}
newDOF <- rssScaled %>%
  filter(applicableForTesting) %>%
  group_by(dataset) %>%
  do(
    data.frame(
      DOF1 = MASS::fitdistr(x = .$rssDiff, densfun = "chi-squared", start = list(df = 1))[["estimate"]],
      DOF2 = MASS::fitdistr(x = .$rss1, densfun = "chi-squared", start = list(df = 1))[["estimate"]]
    ))

newDOF %>% kable()
```

Finally, we can compute the test statistics according to Equation (\ref{eq:f_stat}) and compare them to the F-distribution:

```{r compute_F}
newFStatistics <- rssScaled %>%
  filter(applicableForTesting) %>%
  left_join(newDOF, by = "dataset") %>%
  mutate(fStat = (rssDiff/DOF1) / (rss1/DOF2),
         pVal = 1 - pf(fStat, df1 = DOF1, df2 = DOF2)) %>%
  group_by(dataset) %>%
  mutate(pAdj = p.adjust(pVal, "BH"))
```

We plot the F-statistics against the theoretical F-distribution to check how well the null distribution is approximated now:

```{r plotFnew}
ggplot(newFStatistics) +
  geom_density(aes(x = fStat), fill = "steelblue", alpha = 0.5) +
  geom_line(aes(x = fStat, y = df(fStat, df1 = DOF1, df2 = DOF2)), color = "darkred", size = 1.5) +
  facet_wrap(~ dataset) +
  theme_bw() +
  # Zoom in to small values to increase resolution for the proteins under H0:
  xlim(c(0, 10))
```

Also check the p-value histograms. We expect the null cases to follow a uniform distribution, with a peak on the left for the non-null cases:
```{r plotPnew}
ggplot(newFStatistics) +
  geom_histogram(aes(x = pVal, y = ..density..), fill = "steelblue", alpha = 0.5, boundary = 0) +
  geom_line(aes(x = pVal, y = dunif(pVal)), color = "darkred", size = 1.5) +
  facet_wrap(~ dataset) +
  theme_bw()
```

The F-statistics and p-values approximate the expected distributions substantially closer when based on the estimated degrees of freedom than when based on the theoretical degrees of freedom.

## Detect significantly shifted proteins

Finally, we can select proteins that are significantly shifted by putting a threshold on the Benjamini-Hochberg corrected p-values.

```{r selectHits}
topHits <- newFStatistics %>% 
  filter(pAdj <= 0.01) %>%
  dplyr::select(dataset, uniqueID, fStat, pVal, pAdj) %>%
  arrange(-fStat) %>%
  nest(-dataset) 
```

The table `topHits` contains a list-column `data` with separate data frames for all proteins with Benjamini-Hochberg corrected p-values $\leq 0.01$.

How many proteins were found per dataset?
```{r countHits}
topHits %>%
  mutate(n = map(data, nrow)) %>%
  unnest(n)
```

Let us look at the targets detected in each dataset. The same proteins as shown in Fig. S3, S4, S6, and S7 of the paper.
```{r showHits, results='asis'}
lapply(topHits$data %>% set_names(topHits$dataset), kable)
```

# Session info

```{r session}
devtools::session_info()
```

# Bibliography